{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis(SA)\n",
    "\n",
    "- `knowledge-based techniques` : 단어들의 감정의 정도를 평가하는 사전을 만들고 이를 활용해서 글의 감정 상태를 평가하는 방법\n",
    "    - 예) SentiWordnet : 단어마다의 긍/부정 척도를 더한 대표적인 DB (Pyrhon의 nltk에 포함)\n",
    "- `statistical methods` : SVM(Support Vector Machine)이나 BOW(Bag Of Words)등 다양한 방법론을 활용해서 단어의 감정을 분류하는 방법\n",
    "    - 예) Naive Bayes Classifier (Python nltk에 포함)\n",
    "- `hybrid approaches` : 위의 두 방법을 적절하게 혼합하여 활용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### knowledge-based techniques\n",
    "dataset 출처 : http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir('../../aclImdb/train/pos')\n",
    "\n",
    "first_file = files[0]\n",
    "with open('../../aclImdb/train/pos/{}'.format(first_file), 'r', encoding='utf-8') as f:\n",
    "    review = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train_list = []\n",
    "for file in files:\n",
    "    with open('../../aclImdb/train/pos/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        review = f.read()\n",
    "    pos_train_list.append(review)\n",
    "len(pos_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/macbook/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('hate.n.01'), SentiSynset('hate.v.01')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('hate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('hate', 'v'))[0].pos_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('hate', 'v'))[0].neg_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 유의어의 긍/부정 스코어를 평균인 것으로 쓰기 위한 함수\n",
    "def word_sentiment_calculator(word, tag):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    \n",
    "    # syn_set : 유의어 집합\n",
    "    if 'NN' in tag and len(list(swn.senti_synsets(word, 'n'))) > 0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'n'))\n",
    "    elif 'VB' in tag and len(list(swn.senti_synsets(word, 'v'))) > 0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'v'))\n",
    "    elif 'JJ' in tag and len(list(swn.senti_synsets(word, 'a'))) > 0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'a'))\n",
    "    elif 'RB' in tag and len(list(swn.senti_synsets(word, 'r'))) > 0:\n",
    "        syn_set = list(swn.senti_synsets(word, 'r'))\n",
    "    else:\n",
    "        return(0, 0)\n",
    "    \n",
    "    for syn in syn_set:\n",
    "        pos_score += syn.pos_score()\n",
    "        neg_score += syn.neg_score()\n",
    "    return (pos_score / len(syn_set), neg_score / len(syn_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.625, 0.03125)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentiment_calculator('love', 'VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에서의 긍/부정 지수 계산함수\n",
    "def sentence_sentiment_calculator(pos_tags):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    for word, tag in pos_tags:\n",
    "        pos_score += word_sentiment_calculator(word, tag)[0]\n",
    "        neg_score += word_sentiment_calculator(word, tag)[1]\n",
    "    return (pos_score, neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]\n",
      "Number of correct instance: 12\n",
      "Number of incorrect instance: 8\n"
     ]
    }
   ],
   "source": [
    "pos_files = os.listdir('../../aclImdb/train/pos/')[:10]\n",
    "neg_files = os.listdir('../../aclImdb/train/neg/')[:10]\n",
    "\n",
    "# 긍정적리뷰 10개 부정적리뷰 10개를 순서대로 불러왔으므로,,,\n",
    "actual = [1] * 10 + [0] * 10\n",
    "predicted = []\n",
    "\n",
    "def sentence_sentiment_calculator2(pos_tags):\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    s_tk = nltk.word_tokenize(pos_tags)\n",
    "    pos_tags = nltk.pos_tag(s_tk)\n",
    "    for word, tag in pos_tags:\n",
    "        pos_score += word_sentiment_calculator(word, tag)[0]\n",
    "        neg_score += word_sentiment_calculator(word, tag)[1]\n",
    "    return (pos_score, neg_score)\n",
    "\n",
    "for file in pos_files:\n",
    "    with open('../../aclImdb/train/pos/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        scores = sentence_sentiment_calculator2(f.read())\n",
    "        \n",
    "        if scores[0] >= scores[1]:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "            \n",
    "for file in neg_files:\n",
    "    with open('../../aclImdb/train/neg/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        scores = sentence_sentiment_calculator2(f.read())\n",
    "        \n",
    "        if scores[0] >= scores[1]:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "            \n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for i in range(20):\n",
    "    if actual[i]  == predicted[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        \n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "print('Number of correct instance:', correct)\n",
    "print('Number of incorrect instance:', incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### statistical methods\n",
    "- Naive Bayes Classifier\n",
    "    - TF-IDF(Term Frequency - Inverse Document Frequency)\n",
    "        - 단어의 등장여부를 T/F로 보는 boolean model\n",
    "        - 단어의 등장횟수로 보는 frequency model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2293713\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir('../../aclImdb/train/pos')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# 긍정 학습데이터에서 모든 토큰을 words라는 리스트에 저장\n",
    "words = []\n",
    "for file in files:\n",
    "    with open('../../aclImdb/train/pos/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        review = nltk.word_tokenize(f.read())\n",
    "        for token in review:\n",
    "            # stopwords 제거\n",
    "            if token not in stopWords:\n",
    "                words.append(token)\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4557794"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 부정 학습데이터도 같은 방식으로 words 리스트에 저장\n",
    "files = os.listdir('../../aclImdb/train/neg')\n",
    "for file in files:\n",
    "    with open('../../aclImdb/train/neg/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        review = nltk.word_tokenize(f.read())\n",
    "        for token in review:\n",
    "            if token not in stopWords:\n",
    "                words.append(token)\n",
    "                \n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words 리스트를 이용해서 3000 dimension의 word feature를 생성\n",
    "words = nltk.FreqDist(words)\n",
    "word_features = list(words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bromwell',\n",
       " 'High',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " '.',\n",
       " 'It',\n",
       " 'ran',\n",
       " 'time',\n",
       " 'programs',\n",
       " 'school',\n",
       " 'life',\n",
       " ',',\n",
       " '``',\n",
       " 'Teachers',\n",
       " \"''\",\n",
       " 'My',\n",
       " '35',\n",
       " 'years',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'believe',\n",
       " \"'s\",\n",
       " 'satire',\n",
       " 'much',\n",
       " 'closer',\n",
       " 'reality',\n",
       " 'The',\n",
       " 'scramble',\n",
       " 'survive',\n",
       " 'financially',\n",
       " 'insightful',\n",
       " 'students',\n",
       " 'see',\n",
       " 'right',\n",
       " 'pathetic',\n",
       " 'teachers',\n",
       " \"'\",\n",
       " 'pomp',\n",
       " 'pettiness',\n",
       " 'whole',\n",
       " 'situation',\n",
       " 'remind',\n",
       " 'schools',\n",
       " 'I',\n",
       " 'knew',\n",
       " 'When',\n",
       " 'saw',\n",
       " 'episode',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'tried',\n",
       " 'burn',\n",
       " 'immediately',\n",
       " 'recalled',\n",
       " '...',\n",
       " 'A',\n",
       " 'classic',\n",
       " 'line',\n",
       " ':',\n",
       " 'INSPECTOR',\n",
       " \"'m\",\n",
       " 'sack',\n",
       " 'one',\n",
       " 'STUDENT',\n",
       " 'Welcome',\n",
       " 'expect',\n",
       " 'many',\n",
       " 'adults',\n",
       " 'age',\n",
       " 'think',\n",
       " 'far',\n",
       " 'fetched',\n",
       " 'What',\n",
       " 'pity',\n",
       " \"n't\",\n",
       " '!',\n",
       " 'Homelessness',\n",
       " '(',\n",
       " 'Houselessness',\n",
       " 'George',\n",
       " 'Carlin',\n",
       " 'stated',\n",
       " ')',\n",
       " 'issue',\n",
       " 'never',\n",
       " 'plan',\n",
       " 'help',\n",
       " 'street',\n",
       " 'considered',\n",
       " 'human',\n",
       " 'everything',\n",
       " 'going',\n",
       " 'work',\n",
       " 'vote',\n",
       " 'matter',\n",
       " 'Most',\n",
       " 'people',\n",
       " 'homeless',\n",
       " 'lost',\n",
       " 'cause',\n",
       " 'worrying',\n",
       " 'things',\n",
       " 'racism',\n",
       " 'war',\n",
       " 'Iraq',\n",
       " 'pressuring',\n",
       " 'kids',\n",
       " 'succeed',\n",
       " 'technology',\n",
       " 'elections',\n",
       " 'inflation',\n",
       " \"'ll\",\n",
       " 'next',\n",
       " 'end',\n",
       " 'streets.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'But',\n",
       " 'given',\n",
       " 'bet',\n",
       " 'live',\n",
       " 'streets',\n",
       " 'month',\n",
       " 'without',\n",
       " 'luxuries',\n",
       " 'home',\n",
       " 'entertainment',\n",
       " 'sets',\n",
       " 'bathroom',\n",
       " 'pictures',\n",
       " 'wall',\n",
       " 'computer',\n",
       " 'treasure',\n",
       " 'like',\n",
       " '?',\n",
       " 'That',\n",
       " 'Goddard',\n",
       " 'Bolt',\n",
       " 'lesson.',\n",
       " 'Mel',\n",
       " 'Brooks',\n",
       " 'directs',\n",
       " 'stars',\n",
       " 'plays',\n",
       " 'rich',\n",
       " 'man',\n",
       " 'world',\n",
       " 'deciding',\n",
       " 'make',\n",
       " 'sissy',\n",
       " 'rival',\n",
       " 'Jeffery',\n",
       " 'Tambor',\n",
       " 'thirty',\n",
       " 'days',\n",
       " ';',\n",
       " 'succeeds',\n",
       " 'wants',\n",
       " 'future',\n",
       " 'project',\n",
       " 'making',\n",
       " 'buildings',\n",
       " 'thrown',\n",
       " 'bracelet',\n",
       " 'leg',\n",
       " 'monitor',\n",
       " 'every',\n",
       " 'move',\n",
       " 'ca',\n",
       " 'step',\n",
       " 'sidewalk',\n",
       " 'He',\n",
       " 'nickname',\n",
       " 'Pepto',\n",
       " 'vagrant',\n",
       " 'written',\n",
       " 'forehead',\n",
       " 'meets',\n",
       " 'characters',\n",
       " 'including',\n",
       " 'woman',\n",
       " 'name',\n",
       " 'Molly',\n",
       " 'Lesley',\n",
       " 'Ann',\n",
       " 'Warren',\n",
       " 'ex-dancer',\n",
       " 'got',\n",
       " 'divorce',\n",
       " 'losing',\n",
       " 'pals',\n",
       " 'Sailor',\n",
       " 'Howard',\n",
       " 'Morris',\n",
       " 'Fumes',\n",
       " 'Teddy',\n",
       " 'Wilson',\n",
       " 'already',\n",
       " 'used',\n",
       " 'They',\n",
       " \"'re\",\n",
       " 'survivors',\n",
       " 'reaching',\n",
       " 'mutual',\n",
       " 'agreements',\n",
       " 'fight',\n",
       " 'flight',\n",
       " 'kill',\n",
       " 'killed.',\n",
       " 'While',\n",
       " 'love',\n",
       " 'connection',\n",
       " 'necessary',\n",
       " 'plot',\n",
       " 'found',\n",
       " 'Life',\n",
       " 'Stinks',\n",
       " 'observant',\n",
       " 'films',\n",
       " 'prior',\n",
       " 'shows',\n",
       " 'tender',\n",
       " 'side',\n",
       " 'compared',\n",
       " 'slapstick',\n",
       " 'Blazing',\n",
       " 'Saddles',\n",
       " 'Young',\n",
       " 'Frankenstein',\n",
       " 'Spaceballs',\n",
       " 'show',\n",
       " 'something',\n",
       " 'valuable',\n",
       " 'day',\n",
       " 'hand',\n",
       " 'stupid',\n",
       " 'know',\n",
       " 'money',\n",
       " 'Maybe',\n",
       " 'give',\n",
       " 'instead',\n",
       " 'using',\n",
       " 'Monopoly',\n",
       " 'money.',\n",
       " 'Or',\n",
       " 'maybe',\n",
       " 'film',\n",
       " 'inspire',\n",
       " 'others',\n",
       " 'Brilliant',\n",
       " 'over-acting',\n",
       " 'Best',\n",
       " 'dramatic',\n",
       " 'hobo',\n",
       " 'lady',\n",
       " 'ever',\n",
       " 'seen',\n",
       " 'scenes',\n",
       " 'clothes',\n",
       " 'warehouse',\n",
       " 'second',\n",
       " 'none',\n",
       " 'corn',\n",
       " 'face',\n",
       " 'good',\n",
       " 'anything',\n",
       " 'take',\n",
       " 'lawyers',\n",
       " 'also',\n",
       " 'superb',\n",
       " 'After',\n",
       " 'accused',\n",
       " 'turncoat',\n",
       " 'selling',\n",
       " 'boss',\n",
       " 'dishonest',\n",
       " 'lawyer',\n",
       " 'shrugs',\n",
       " 'indifferently',\n",
       " 'says',\n",
       " 'Three',\n",
       " 'funny',\n",
       " 'words',\n",
       " 'Jeffrey',\n",
       " 'favorite',\n",
       " 'later',\n",
       " 'Larry',\n",
       " 'Sanders',\n",
       " 'fantastic',\n",
       " 'mad',\n",
       " 'millionaire',\n",
       " 'crush',\n",
       " 'ghetto',\n",
       " 'His',\n",
       " 'character',\n",
       " 'malevolent',\n",
       " 'usual',\n",
       " 'hospital',\n",
       " 'scene',\n",
       " 'invade',\n",
       " 'demolition',\n",
       " 'site',\n",
       " 'all-time',\n",
       " 'classics',\n",
       " 'Look',\n",
       " 'legs',\n",
       " 'two',\n",
       " 'big',\n",
       " 'diggers',\n",
       " 'fighting',\n",
       " 'bleeds',\n",
       " 'This',\n",
       " 'movie',\n",
       " 'gets',\n",
       " 'better',\n",
       " 'quite',\n",
       " 'often',\n",
       " 'easily',\n",
       " 'underrated',\n",
       " 'inn',\n",
       " 'cannon',\n",
       " 'Sure',\n",
       " 'flawed',\n",
       " 'realistic',\n",
       " 'view',\n",
       " 'homelessness',\n",
       " 'unlike',\n",
       " 'say',\n",
       " 'Citizen',\n",
       " 'Kane',\n",
       " 'gave',\n",
       " 'lounge',\n",
       " 'singers',\n",
       " 'Titanic',\n",
       " 'Italians',\n",
       " 'YOU',\n",
       " 'IDIOTS',\n",
       " 'Many',\n",
       " 'jokes',\n",
       " 'fall',\n",
       " 'flat',\n",
       " 'still',\n",
       " 'lovable',\n",
       " 'way',\n",
       " 'comedies',\n",
       " 'pull',\n",
       " 'story',\n",
       " 'traditionally',\n",
       " 'reviled',\n",
       " 'members',\n",
       " 'society',\n",
       " 'truly',\n",
       " 'impressive',\n",
       " 'Its',\n",
       " 'Fisher',\n",
       " 'King',\n",
       " 'crap',\n",
       " 'either',\n",
       " 'complaint',\n",
       " 'cast',\n",
       " 'someone',\n",
       " 'else',\n",
       " 'Director',\n",
       " 'Writer',\n",
       " 'typical',\n",
       " 'less',\n",
       " 'movies',\n",
       " 'actually',\n",
       " 'followable',\n",
       " 'Leslie',\n",
       " 'made',\n",
       " 'under-rated',\n",
       " 'actress',\n",
       " 'There',\n",
       " 'moments',\n",
       " 'could',\n",
       " 'fleshed',\n",
       " 'bit',\n",
       " 'probably',\n",
       " 'cut',\n",
       " 'room',\n",
       " 'worth',\n",
       " 'price',\n",
       " 'rent',\n",
       " 'acting',\n",
       " 'overall',\n",
       " 'job',\n",
       " 'characteristic',\n",
       " 'speaking',\n",
       " 'directly',\n",
       " 'audience',\n",
       " 'Again',\n",
       " 'best',\n",
       " 'actor',\n",
       " 'Fume',\n",
       " 'played',\n",
       " 'parts',\n",
       " 'well',\n",
       " 'comedic',\n",
       " 'Robin',\n",
       " 'Williams',\n",
       " 'quirky/insane',\n",
       " 'recent',\n",
       " 'thriller',\n",
       " 'fame',\n",
       " 'hybrid',\n",
       " 'drama',\n",
       " 'over-dramatization',\n",
       " 'mixed',\n",
       " 'new',\n",
       " 'per',\n",
       " 'se',\n",
       " 'mystery/suspense',\n",
       " 'vehicle',\n",
       " 'attempts',\n",
       " 'locate',\n",
       " 'sick',\n",
       " 'boy',\n",
       " 'keeper.',\n",
       " 'Also',\n",
       " 'starring',\n",
       " 'Sandra',\n",
       " 'Oh',\n",
       " 'Rory',\n",
       " 'Culkin',\n",
       " 'Suspense',\n",
       " 'Drama',\n",
       " 'pretty',\n",
       " 'news',\n",
       " 'report',\n",
       " 'William',\n",
       " 'close',\n",
       " 'achieving',\n",
       " 'goal.',\n",
       " 'must',\n",
       " 'highly',\n",
       " 'entertained',\n",
       " 'though',\n",
       " 'fails',\n",
       " 'teach',\n",
       " 'guide',\n",
       " 'inspect',\n",
       " 'amuse',\n",
       " 'felt',\n",
       " 'watching',\n",
       " 'guy',\n",
       " 'performing',\n",
       " 'actions',\n",
       " 'third',\n",
       " 'person',\n",
       " 'perspective',\n",
       " 'In',\n",
       " 'real',\n",
       " 'able',\n",
       " 'subscribe',\n",
       " 'premise',\n",
       " 'story.',\n",
       " 'All',\n",
       " 'watch',\n",
       " 'definitely',\n",
       " 'Friday/Saturday',\n",
       " 'night',\n",
       " 'fare.',\n",
       " 'rates',\n",
       " '7.7/10',\n",
       " 'Fiend',\n",
       " 'Yes',\n",
       " 'art',\n",
       " 'successfully',\n",
       " 'slow',\n",
       " 'paced',\n",
       " 'thriller.',\n",
       " 'unfolds',\n",
       " 'nice',\n",
       " 'volumes',\n",
       " 'even',\n",
       " 'notice',\n",
       " 'happening.',\n",
       " 'Fine',\n",
       " 'performance',\n",
       " 'sexuality',\n",
       " 'angles',\n",
       " 'seem',\n",
       " 'unnecessary',\n",
       " 'affect',\n",
       " 'enjoy',\n",
       " 'However',\n",
       " 'core',\n",
       " 'engaging',\n",
       " 'rush',\n",
       " 'onto',\n",
       " 'grips',\n",
       " 'enough',\n",
       " 'keep',\n",
       " 'wondering',\n",
       " 'direction',\n",
       " 'Use',\n",
       " 'lights',\n",
       " 'achieve',\n",
       " 'desired',\n",
       " 'affects',\n",
       " 'suspense',\n",
       " 'unexpectedness',\n",
       " 'good.',\n",
       " 'Very',\n",
       " '1',\n",
       " 'looking',\n",
       " 'lay',\n",
       " 'back',\n",
       " 'hear',\n",
       " 'thrilling',\n",
       " 'short',\n",
       " 'critically',\n",
       " 'acclaimed',\n",
       " 'psychological',\n",
       " 'based',\n",
       " 'true',\n",
       " 'events',\n",
       " 'Gabriel',\n",
       " 'celebrated',\n",
       " 'writer',\n",
       " 'late-night',\n",
       " 'talk',\n",
       " 'host',\n",
       " 'becomes',\n",
       " 'captivated',\n",
       " 'harrowing',\n",
       " 'young',\n",
       " 'listener',\n",
       " 'adoptive',\n",
       " 'mother',\n",
       " 'Toni',\n",
       " 'Collette',\n",
       " 'troubling',\n",
       " 'questions',\n",
       " 'arise',\n",
       " 'however',\n",
       " 'finds',\n",
       " 'drawn',\n",
       " 'widening',\n",
       " 'mystery',\n",
       " 'hides',\n",
       " 'deadly',\n",
       " 'secret',\n",
       " 'according',\n",
       " 'official',\n",
       " 'synopsis.',\n",
       " 'You',\n",
       " 'really',\n",
       " 'STOP',\n",
       " 'reading',\n",
       " 'comments',\n",
       " 'NOW',\n",
       " 'How',\n",
       " 'lose',\n",
       " 'ending',\n",
       " 'Ms.',\n",
       " 'planning',\n",
       " 'chopped',\n",
       " 'sent',\n",
       " 'deleted',\n",
       " 'land',\n",
       " 'overkill',\n",
       " 'nature',\n",
       " 'physical',\n",
       " 'mental',\n",
       " 'ailments',\n",
       " 'obvious',\n",
       " 'Mr.',\n",
       " 'returns',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Possibly',\n",
       " 'blindness',\n",
       " 'question',\n",
       " '-',\n",
       " 'revelation',\n",
       " 'certain',\n",
       " 'highway',\n",
       " 'video',\n",
       " 'tape',\n",
       " 'would',\n",
       " 'benefit',\n",
       " 're-editing',\n",
       " 'director',\n",
       " 'Bobby',\n",
       " 'Cannavale',\n",
       " 'Jess',\n",
       " 'initially',\n",
       " 'believable',\n",
       " 'couple',\n",
       " 'establishing',\n",
       " 'relationship',\n",
       " 'might',\n",
       " 'helped',\n",
       " 'set',\n",
       " 'stage',\n",
       " 'Otherwise',\n",
       " 'exemplary',\n",
       " 'offers',\n",
       " 'exceptionally',\n",
       " 'strong',\n",
       " 'characterization',\n",
       " 'gay',\n",
       " 'impersonation',\n",
       " 'Anna',\n",
       " 'Joe',\n",
       " 'Morton',\n",
       " 'Ashe',\n",
       " 'Pete',\n",
       " 'Logand',\n",
       " 'perfect.',\n",
       " 'Donna',\n",
       " 'belongs',\n",
       " 'creepy',\n",
       " 'hall',\n",
       " 'correct',\n",
       " 'saying',\n",
       " \"'Psycho\",\n",
       " 'several',\n",
       " 'organizations',\n",
       " 'giving',\n",
       " 'awards',\n",
       " 'seemed',\n",
       " 'reach',\n",
       " 'women',\n",
       " 'due',\n",
       " 'slighter',\n",
       " 'dispersion',\n",
       " 'roles',\n",
       " 'certainly',\n",
       " 'noticed',\n",
       " 'award',\n",
       " 'consideration',\n",
       " 'She',\n",
       " 'And',\n",
       " 'Patrick',\n",
       " 'Stettner',\n",
       " 'evokes',\n",
       " 'Hitchcock',\n",
       " 'makes',\n",
       " 'getting',\n",
       " 'sandwich',\n",
       " 'vending',\n",
       " 'machine',\n",
       " 'suspenseful.',\n",
       " 'Finally',\n",
       " 'writers',\n",
       " 'Armistead',\n",
       " 'Maupin',\n",
       " 'Terry',\n",
       " 'Anderson',\n",
       " 'deserve',\n",
       " 'gratitude',\n",
       " 'attendants',\n",
       " 'everywhere.',\n",
       " '*******',\n",
       " 'Night',\n",
       " 'Listener',\n",
       " '1/21/06',\n",
       " '~',\n",
       " 'THE',\n",
       " 'NIGHT',\n",
       " 'LISTENER',\n",
       " '2006',\n",
       " '**1/2',\n",
       " 'John',\n",
       " 'Cullum',\n",
       " 'Lisa',\n",
       " 'Emery',\n",
       " 'Becky',\n",
       " 'Baker',\n",
       " 'Dir',\n",
       " 'Hitchcockian',\n",
       " 'suspenser',\n",
       " 'gives',\n",
       " 'stand-out',\n",
       " 'low-key',\n",
       " 'performance.',\n",
       " 'celebrities',\n",
       " 'fans',\n",
       " 'near',\n",
       " 'paranoia',\n",
       " 'associates',\n",
       " 'almost',\n",
       " 'norm',\n",
       " 'latest',\n",
       " 'derange',\n",
       " 'fan',\n",
       " 'scenario',\n",
       " 'talk-radio',\n",
       " 'personality',\n",
       " 'named',\n",
       " 'No',\n",
       " 'reads',\n",
       " 'stories',\n",
       " 'penned',\n",
       " 'airwaves',\n",
       " 'accumulated',\n",
       " 'interesting',\n",
       " 'form',\n",
       " 'submitted',\n",
       " 'manuscript',\n",
       " 'travails',\n",
       " 'troubled',\n",
       " 'youth',\n",
       " 'editor',\n",
       " 'read',\n",
       " 'naturally',\n",
       " 'disturbed',\n",
       " 'ultimately',\n",
       " 'intrigued',\n",
       " 'nightmarish',\n",
       " 'existence',\n",
       " 'abducted',\n",
       " 'sexually',\n",
       " 'abused',\n",
       " 'finally',\n",
       " 'rescued',\n",
       " 'nurse',\n",
       " 'excellent',\n",
       " 'adopted',\n",
       " 'correspondence',\n",
       " 'reveals',\n",
       " 'dying',\n",
       " 'AIDS',\n",
       " 'Naturally',\n",
       " 'meet',\n",
       " 'suddenly',\n",
       " 'doubt',\n",
       " 'possibly',\n",
       " 'devious',\n",
       " 'ulterior',\n",
       " 'motives',\n",
       " 'seed',\n",
       " 'planted',\n",
       " 'estranged',\n",
       " 'lover',\n",
       " 'whose',\n",
       " 'sudden',\n",
       " 'departure',\n",
       " 'City',\n",
       " 'apartment',\n",
       " 'emotional',\n",
       " 'tailspin',\n",
       " 'grown',\n",
       " 'tempest',\n",
       " 'teacup',\n",
       " 'decides',\n",
       " 'investigating',\n",
       " 'backgrounds',\n",
       " 'discovering',\n",
       " 'truths',\n",
       " 'anticipate.',\n",
       " 'Written',\n",
       " 'co-wrote',\n",
       " 'screenplay',\n",
       " 'former',\n",
       " 'novice',\n",
       " 'hoax',\n",
       " 'run',\n",
       " 'full',\n",
       " 'tilt',\n",
       " 'old',\n",
       " 'fashioned',\n",
       " 'pot-boiler',\n",
       " 'helps',\n",
       " 'conflicted',\n",
       " 'good-hearted',\n",
       " 'genuinely',\n",
       " 'number',\n",
       " 'fact',\n",
       " 'thing',\n",
       " 'escaped',\n",
       " 'unsettling',\n",
       " 'dreadful',\n",
       " 'trait',\n",
       " 'leave',\n",
       " 'unmentioned',\n",
       " 'underlines',\n",
       " 'desperation',\n",
       " 'rattle',\n",
       " 'core.',\n",
       " 'runs',\n",
       " 'gas',\n",
       " 'eventually',\n",
       " 'repetitive',\n",
       " 'predictable',\n",
       " 'despite',\n",
       " 'finely',\n",
       " 'directed',\n",
       " 'piece',\n",
       " 'hoodwink',\n",
       " 'pays',\n",
       " 'listen',\n",
       " 'inner',\n",
       " 'voice',\n",
       " 'careful',\n",
       " 'hope',\n",
       " 'God',\n",
       " 'bless',\n",
       " 'constantly',\n",
       " 'shooting',\n",
       " 'foot',\n",
       " 'lately',\n",
       " 'dumb',\n",
       " 'done',\n",
       " 'decade',\n",
       " 'perhaps',\n",
       " 'exception',\n",
       " 'Death',\n",
       " 'To',\n",
       " 'Smoochy',\n",
       " 'bombed',\n",
       " 'came',\n",
       " 'cult',\n",
       " 'dramas',\n",
       " 'especially',\n",
       " 'Insomnia',\n",
       " 'One',\n",
       " 'Hour',\n",
       " 'Photo',\n",
       " 'mediocre',\n",
       " 'reviews',\n",
       " 'quick',\n",
       " 'DVD',\n",
       " 'release',\n",
       " 'among',\n",
       " 'period.',\n",
       " 'chilling',\n",
       " 'include',\n",
       " 'serial',\n",
       " 'killer',\n",
       " 'anyone',\n",
       " 'physically',\n",
       " 'dangerous',\n",
       " 'concept',\n",
       " 'actual',\n",
       " 'case',\n",
       " 'fraud',\n",
       " 'yet',\n",
       " 'officially',\n",
       " 'confirmed',\n",
       " 'high',\n",
       " 'autobiography',\n",
       " 'child',\n",
       " 'Anthony',\n",
       " 'Godby',\n",
       " 'Johnson',\n",
       " 'suffered',\n",
       " 'horrific',\n",
       " 'abuse',\n",
       " 'contracted',\n",
       " 'result',\n",
       " 'moved',\n",
       " 'reports',\n",
       " 'online',\n",
       " 'may',\n",
       " 'exist',\n",
       " 'confused',\n",
       " 'feelings',\n",
       " 'brilliantly',\n",
       " 'portrayed',\n",
       " 'resurfaced',\n",
       " 'mind.',\n",
       " 'sociopathic',\n",
       " 'caretaker',\n",
       " 'Her',\n",
       " 'role',\n",
       " 'cry',\n",
       " 'Little',\n",
       " 'Miss',\n",
       " 'Sunshine',\n",
       " 'times',\n",
       " 'looked',\n",
       " 'camera',\n",
       " 'thought',\n",
       " 'staring',\n",
       " 'takes',\n",
       " 'play',\n",
       " 'sort',\n",
       " 'understated',\n",
       " 'reviewed',\n",
       " 'actresses',\n",
       " 'generation',\n",
       " 'nominated',\n",
       " 'Academy',\n",
       " 'Award',\n",
       " '2008',\n",
       " 'incredible',\n",
       " 'least',\n",
       " 'scary',\n",
       " 'too.',\n",
       " 'dark',\n",
       " 'recommend',\n",
       " 'Be',\n",
       " 'prepared',\n",
       " 'unsettled',\n",
       " 'leaves',\n",
       " 'strange',\n",
       " 'feeling',\n",
       " 'first',\n",
       " 'Maupins',\n",
       " 'taken',\n",
       " 'displayed',\n",
       " 'cares',\n",
       " 'loves',\n",
       " 'said',\n",
       " 'version',\n",
       " 'expected',\n",
       " 'past',\n",
       " 'gloss',\n",
       " 'Hollywood',\n",
       " 'succeeded',\n",
       " 'With',\n",
       " 'amount',\n",
       " 'restraint',\n",
       " 'captures',\n",
       " 'fragile',\n",
       " 'essence',\n",
       " 'lets',\n",
       " 'us',\n",
       " 'struggle',\n",
       " 'issues',\n",
       " 'trust',\n",
       " 'personnel',\n",
       " 'around',\n",
       " 'As',\n",
       " 'introduced',\n",
       " 'players',\n",
       " 'reminded',\n",
       " 'nothing',\n",
       " 'seems',\n",
       " 'smallest',\n",
       " 'event',\n",
       " 'change',\n",
       " 'lives',\n",
       " 'irrevocably',\n",
       " 'request',\n",
       " 'review',\n",
       " 'book',\n",
       " 'turns',\n",
       " 'changing',\n",
       " 'find',\n",
       " 'strength',\n",
       " 'within',\n",
       " 'carry',\n",
       " 'forward.',\n",
       " 'bad',\n",
       " 'avoid',\n",
       " 'average',\n",
       " 'American',\n",
       " 'serious',\n",
       " 'PLEASE',\n",
       " 'GIVE',\n",
       " 'THIS',\n",
       " 'MOVIE',\n",
       " 'CHANCE',\n",
       " 'touches',\n",
       " 'darkness',\n",
       " 'go',\n",
       " 'Like',\n",
       " 'stepped',\n",
       " 'another',\n",
       " 'quality',\n",
       " 'art.',\n",
       " 'forget',\n",
       " 'steals',\n",
       " '1940',\n",
       " 'leading',\n",
       " 'looks',\n",
       " 'screen',\n",
       " 'presence',\n",
       " 'hacks',\n",
       " 'opinion',\n",
       " 'S~',\n",
       " 'liked',\n",
       " 'Some',\n",
       " 'action',\n",
       " 'tense',\n",
       " 'opening',\n",
       " 'semi',\n",
       " 'truck',\n",
       " 'done.',\n",
       " 'transitional',\n",
       " 'filmed',\n",
       " 'ways',\n",
       " 'lapse',\n",
       " 'photography',\n",
       " 'unusual',\n",
       " 'colors',\n",
       " 'evil',\n",
       " \"'d\",\n",
       " '8',\n",
       " '10',\n",
       " 'illnesses',\n",
       " 'born',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word feature를 통해서 분석할 데이터의 feature를 생성하는 함수\n",
    "def find_features(doc):\n",
    "    words = set(doc)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bromwell': False,\n",
       " 'High': False,\n",
       " 'cartoon': False,\n",
       " 'comedy': True,\n",
       " '.': True,\n",
       " 'It': False,\n",
       " 'ran': False,\n",
       " 'time': True,\n",
       " 'programs': False,\n",
       " 'school': False,\n",
       " 'life': False,\n",
       " ',': True,\n",
       " '``': False,\n",
       " 'Teachers': False,\n",
       " \"''\": False,\n",
       " 'My': False,\n",
       " '35': False,\n",
       " 'years': False,\n",
       " 'teaching': False,\n",
       " 'profession': False,\n",
       " 'lead': False,\n",
       " 'believe': False,\n",
       " \"'s\": True,\n",
       " 'satire': False,\n",
       " 'much': False,\n",
       " 'closer': False,\n",
       " 'reality': False,\n",
       " 'The': True,\n",
       " 'scramble': False,\n",
       " 'survive': False,\n",
       " 'financially': False,\n",
       " 'insightful': False,\n",
       " 'students': False,\n",
       " 'see': False,\n",
       " 'right': False,\n",
       " 'pathetic': False,\n",
       " 'teachers': False,\n",
       " \"'\": False,\n",
       " 'pomp': False,\n",
       " 'pettiness': False,\n",
       " 'whole': False,\n",
       " 'situation': False,\n",
       " 'remind': False,\n",
       " 'schools': False,\n",
       " 'I': False,\n",
       " 'knew': False,\n",
       " 'When': False,\n",
       " 'saw': False,\n",
       " 'episode': False,\n",
       " 'student': False,\n",
       " 'repeatedly': False,\n",
       " 'tried': False,\n",
       " 'burn': False,\n",
       " 'immediately': False,\n",
       " 'recalled': False,\n",
       " '...': False,\n",
       " 'A': True,\n",
       " 'classic': False,\n",
       " 'line': False,\n",
       " ':': False,\n",
       " 'INSPECTOR': False,\n",
       " \"'m\": False,\n",
       " 'sack': False,\n",
       " 'one': False,\n",
       " 'STUDENT': False,\n",
       " 'Welcome': False,\n",
       " 'expect': False,\n",
       " 'many': False,\n",
       " 'adults': False,\n",
       " 'age': False,\n",
       " 'think': True,\n",
       " 'far': False,\n",
       " 'fetched': False,\n",
       " 'What': False,\n",
       " 'pity': False,\n",
       " \"n't\": False,\n",
       " '!': False,\n",
       " 'Homelessness': False,\n",
       " '(': False,\n",
       " 'Houselessness': False,\n",
       " 'George': False,\n",
       " 'Carlin': False,\n",
       " 'stated': False,\n",
       " ')': False,\n",
       " 'issue': False,\n",
       " 'never': False,\n",
       " 'plan': False,\n",
       " 'help': False,\n",
       " 'street': False,\n",
       " 'considered': False,\n",
       " 'human': False,\n",
       " 'everything': False,\n",
       " 'going': False,\n",
       " 'work': False,\n",
       " 'vote': False,\n",
       " 'matter': False,\n",
       " 'Most': False,\n",
       " 'people': False,\n",
       " 'homeless': False,\n",
       " 'lost': False,\n",
       " 'cause': False,\n",
       " 'worrying': False,\n",
       " 'things': False,\n",
       " 'racism': False,\n",
       " 'war': False,\n",
       " 'Iraq': False,\n",
       " 'pressuring': False,\n",
       " 'kids': False,\n",
       " 'succeed': False,\n",
       " 'technology': False,\n",
       " 'elections': False,\n",
       " 'inflation': False,\n",
       " \"'ll\": False,\n",
       " 'next': False,\n",
       " 'end': False,\n",
       " 'streets.': False,\n",
       " '<': False,\n",
       " 'br': False,\n",
       " '/': False,\n",
       " '>': False,\n",
       " 'But': False,\n",
       " 'given': False,\n",
       " 'bet': False,\n",
       " 'live': False,\n",
       " 'streets': False,\n",
       " 'month': False,\n",
       " 'without': False,\n",
       " 'luxuries': False,\n",
       " 'home': False,\n",
       " 'entertainment': False,\n",
       " 'sets': False,\n",
       " 'bathroom': False,\n",
       " 'pictures': False,\n",
       " 'wall': False,\n",
       " 'computer': False,\n",
       " 'treasure': False,\n",
       " 'like': False,\n",
       " '?': False,\n",
       " 'That': False,\n",
       " 'Goddard': False,\n",
       " 'Bolt': False,\n",
       " 'lesson.': False,\n",
       " 'Mel': False,\n",
       " 'Brooks': False,\n",
       " 'directs': False,\n",
       " 'stars': True,\n",
       " 'plays': False,\n",
       " 'rich': False,\n",
       " 'man': True,\n",
       " 'world': False,\n",
       " 'deciding': False,\n",
       " 'make': True,\n",
       " 'sissy': False,\n",
       " 'rival': False,\n",
       " 'Jeffery': False,\n",
       " 'Tambor': False,\n",
       " 'thirty': False,\n",
       " 'days': False,\n",
       " ';': False,\n",
       " 'succeeds': False,\n",
       " 'wants': False,\n",
       " 'future': True,\n",
       " 'project': False,\n",
       " 'making': True,\n",
       " 'buildings': False,\n",
       " 'thrown': False,\n",
       " 'bracelet': False,\n",
       " 'leg': False,\n",
       " 'monitor': False,\n",
       " 'every': False,\n",
       " 'move': False,\n",
       " 'ca': False,\n",
       " 'step': False,\n",
       " 'sidewalk': False,\n",
       " 'He': False,\n",
       " 'nickname': False,\n",
       " 'Pepto': False,\n",
       " 'vagrant': False,\n",
       " 'written': False,\n",
       " 'forehead': False,\n",
       " 'meets': False,\n",
       " 'characters': False,\n",
       " 'including': False,\n",
       " 'woman': False,\n",
       " 'name': False,\n",
       " 'Molly': False,\n",
       " 'Lesley': False,\n",
       " 'Ann': False,\n",
       " 'Warren': False,\n",
       " 'ex-dancer': False,\n",
       " 'got': False,\n",
       " 'divorce': False,\n",
       " 'losing': False,\n",
       " 'pals': False,\n",
       " 'Sailor': False,\n",
       " 'Howard': False,\n",
       " 'Morris': False,\n",
       " 'Fumes': False,\n",
       " 'Teddy': False,\n",
       " 'Wilson': False,\n",
       " 'already': False,\n",
       " 'used': False,\n",
       " 'They': False,\n",
       " \"'re\": False,\n",
       " 'survivors': False,\n",
       " 'reaching': False,\n",
       " 'mutual': False,\n",
       " 'agreements': False,\n",
       " 'fight': False,\n",
       " 'flight': False,\n",
       " 'kill': False,\n",
       " 'killed.': False,\n",
       " 'While': False,\n",
       " 'love': False,\n",
       " 'connection': False,\n",
       " 'necessary': False,\n",
       " 'plot': False,\n",
       " 'found': False,\n",
       " 'Life': False,\n",
       " 'Stinks': False,\n",
       " 'observant': False,\n",
       " 'films': False,\n",
       " 'prior': False,\n",
       " 'shows': False,\n",
       " 'tender': False,\n",
       " 'side': False,\n",
       " 'compared': False,\n",
       " 'slapstick': False,\n",
       " 'Blazing': False,\n",
       " 'Saddles': False,\n",
       " 'Young': False,\n",
       " 'Frankenstein': False,\n",
       " 'Spaceballs': False,\n",
       " 'show': False,\n",
       " 'something': False,\n",
       " 'valuable': False,\n",
       " 'day': False,\n",
       " 'hand': False,\n",
       " 'stupid': False,\n",
       " 'know': False,\n",
       " 'money': False,\n",
       " 'Maybe': False,\n",
       " 'give': False,\n",
       " 'instead': False,\n",
       " 'using': False,\n",
       " 'Monopoly': False,\n",
       " 'money.': False,\n",
       " 'Or': False,\n",
       " 'maybe': False,\n",
       " 'film': False,\n",
       " 'inspire': False,\n",
       " 'others': False,\n",
       " 'Brilliant': False,\n",
       " 'over-acting': False,\n",
       " 'Best': False,\n",
       " 'dramatic': False,\n",
       " 'hobo': False,\n",
       " 'lady': False,\n",
       " 'ever': False,\n",
       " 'seen': True,\n",
       " 'scenes': False,\n",
       " 'clothes': False,\n",
       " 'warehouse': False,\n",
       " 'second': False,\n",
       " 'none': False,\n",
       " 'corn': False,\n",
       " 'face': False,\n",
       " 'good': True,\n",
       " 'anything': False,\n",
       " 'take': False,\n",
       " 'lawyers': False,\n",
       " 'also': False,\n",
       " 'superb': False,\n",
       " 'After': False,\n",
       " 'accused': False,\n",
       " 'turncoat': False,\n",
       " 'selling': False,\n",
       " 'boss': False,\n",
       " 'dishonest': False,\n",
       " 'lawyer': False,\n",
       " 'shrugs': False,\n",
       " 'indifferently': False,\n",
       " 'says': False,\n",
       " 'Three': False,\n",
       " 'funny': False,\n",
       " 'words': False,\n",
       " 'Jeffrey': False,\n",
       " 'favorite': False,\n",
       " 'later': False,\n",
       " 'Larry': False,\n",
       " 'Sanders': False,\n",
       " 'fantastic': False,\n",
       " 'mad': False,\n",
       " 'millionaire': False,\n",
       " 'crush': False,\n",
       " 'ghetto': False,\n",
       " 'His': False,\n",
       " 'character': False,\n",
       " 'malevolent': False,\n",
       " 'usual': False,\n",
       " 'hospital': False,\n",
       " 'scene': True,\n",
       " 'invade': False,\n",
       " 'demolition': False,\n",
       " 'site': False,\n",
       " 'all-time': False,\n",
       " 'classics': False,\n",
       " 'Look': False,\n",
       " 'legs': False,\n",
       " 'two': False,\n",
       " 'big': False,\n",
       " 'diggers': False,\n",
       " 'fighting': False,\n",
       " 'bleeds': False,\n",
       " 'This': False,\n",
       " 'movie': False,\n",
       " 'gets': False,\n",
       " 'better': True,\n",
       " 'quite': False,\n",
       " 'often': False,\n",
       " 'easily': False,\n",
       " 'underrated': False,\n",
       " 'inn': False,\n",
       " 'cannon': False,\n",
       " 'Sure': False,\n",
       " 'flawed': False,\n",
       " 'realistic': False,\n",
       " 'view': False,\n",
       " 'homelessness': False,\n",
       " 'unlike': False,\n",
       " 'say': False,\n",
       " 'Citizen': False,\n",
       " 'Kane': False,\n",
       " 'gave': False,\n",
       " 'lounge': False,\n",
       " 'singers': True,\n",
       " 'Titanic': False,\n",
       " 'Italians': False,\n",
       " 'YOU': False,\n",
       " 'IDIOTS': False,\n",
       " 'Many': False,\n",
       " 'jokes': False,\n",
       " 'fall': False,\n",
       " 'flat': False,\n",
       " 'still': False,\n",
       " 'lovable': False,\n",
       " 'way': False,\n",
       " 'comedies': False,\n",
       " 'pull': False,\n",
       " 'story': False,\n",
       " 'traditionally': False,\n",
       " 'reviled': False,\n",
       " 'members': False,\n",
       " 'society': False,\n",
       " 'truly': False,\n",
       " 'impressive': False,\n",
       " 'Its': False,\n",
       " 'Fisher': False,\n",
       " 'King': False,\n",
       " 'crap': False,\n",
       " 'either': False,\n",
       " 'complaint': False,\n",
       " 'cast': False,\n",
       " 'someone': False,\n",
       " 'else': False,\n",
       " 'Director': False,\n",
       " 'Writer': False,\n",
       " 'typical': False,\n",
       " 'less': False,\n",
       " 'movies': False,\n",
       " 'actually': False,\n",
       " 'followable': False,\n",
       " 'Leslie': False,\n",
       " 'made': False,\n",
       " 'under-rated': False,\n",
       " 'actress': False,\n",
       " 'There': False,\n",
       " 'moments': False,\n",
       " 'could': False,\n",
       " 'fleshed': False,\n",
       " 'bit': False,\n",
       " 'probably': False,\n",
       " 'cut': False,\n",
       " 'room': False,\n",
       " 'worth': False,\n",
       " 'price': False,\n",
       " 'rent': False,\n",
       " 'acting': False,\n",
       " 'overall': False,\n",
       " 'job': False,\n",
       " 'characteristic': False,\n",
       " 'speaking': False,\n",
       " 'directly': False,\n",
       " 'audience': True,\n",
       " 'Again': False,\n",
       " 'best': False,\n",
       " 'actor': False,\n",
       " 'Fume': False,\n",
       " 'played': False,\n",
       " 'parts': False,\n",
       " 'well': False,\n",
       " 'comedic': False,\n",
       " 'Robin': False,\n",
       " 'Williams': False,\n",
       " 'quirky/insane': False,\n",
       " 'recent': False,\n",
       " 'thriller': False,\n",
       " 'fame': False,\n",
       " 'hybrid': False,\n",
       " 'drama': False,\n",
       " 'over-dramatization': False,\n",
       " 'mixed': False,\n",
       " 'new': False,\n",
       " 'per': False,\n",
       " 'se': False,\n",
       " 'mystery/suspense': False,\n",
       " 'vehicle': False,\n",
       " 'attempts': False,\n",
       " 'locate': False,\n",
       " 'sick': False,\n",
       " 'boy': False,\n",
       " 'keeper.': False,\n",
       " 'Also': False,\n",
       " 'starring': False,\n",
       " 'Sandra': False,\n",
       " 'Oh': False,\n",
       " 'Rory': False,\n",
       " 'Culkin': False,\n",
       " 'Suspense': False,\n",
       " 'Drama': False,\n",
       " 'pretty': False,\n",
       " 'news': False,\n",
       " 'report': False,\n",
       " 'William': False,\n",
       " 'close': False,\n",
       " 'achieving': False,\n",
       " 'goal.': False,\n",
       " 'must': False,\n",
       " 'highly': False,\n",
       " 'entertained': False,\n",
       " 'though': False,\n",
       " 'fails': False,\n",
       " 'teach': False,\n",
       " 'guide': False,\n",
       " 'inspect': False,\n",
       " 'amuse': False,\n",
       " 'felt': False,\n",
       " 'watching': False,\n",
       " 'guy': False,\n",
       " 'performing': False,\n",
       " 'actions': False,\n",
       " 'third': True,\n",
       " 'person': False,\n",
       " 'perspective': False,\n",
       " 'In': False,\n",
       " 'real': False,\n",
       " 'able': False,\n",
       " 'subscribe': False,\n",
       " 'premise': False,\n",
       " 'story.': False,\n",
       " 'All': False,\n",
       " 'watch': False,\n",
       " 'definitely': False,\n",
       " 'Friday/Saturday': False,\n",
       " 'night': False,\n",
       " 'fare.': False,\n",
       " 'rates': False,\n",
       " '7.7/10': False,\n",
       " 'Fiend': False,\n",
       " 'Yes': False,\n",
       " 'art': False,\n",
       " 'successfully': False,\n",
       " 'slow': False,\n",
       " 'paced': False,\n",
       " 'thriller.': False,\n",
       " 'unfolds': False,\n",
       " 'nice': False,\n",
       " 'volumes': False,\n",
       " 'even': False,\n",
       " 'notice': False,\n",
       " 'happening.': False,\n",
       " 'Fine': False,\n",
       " 'performance': False,\n",
       " 'sexuality': False,\n",
       " 'angles': False,\n",
       " 'seem': True,\n",
       " 'unnecessary': False,\n",
       " 'affect': False,\n",
       " 'enjoy': False,\n",
       " 'However': False,\n",
       " 'core': False,\n",
       " 'engaging': False,\n",
       " 'rush': False,\n",
       " 'onto': False,\n",
       " 'grips': False,\n",
       " 'enough': False,\n",
       " 'keep': False,\n",
       " 'wondering': False,\n",
       " 'direction': False,\n",
       " 'Use': False,\n",
       " 'lights': False,\n",
       " 'achieve': False,\n",
       " 'desired': False,\n",
       " 'affects': False,\n",
       " 'suspense': False,\n",
       " 'unexpectedness': False,\n",
       " 'good.': False,\n",
       " 'Very': False,\n",
       " '1': False,\n",
       " 'looking': False,\n",
       " 'lay': False,\n",
       " 'back': False,\n",
       " 'hear': False,\n",
       " 'thrilling': False,\n",
       " 'short': False,\n",
       " 'critically': False,\n",
       " 'acclaimed': False,\n",
       " 'psychological': False,\n",
       " 'based': False,\n",
       " 'true': False,\n",
       " 'events': False,\n",
       " 'Gabriel': False,\n",
       " 'celebrated': False,\n",
       " 'writer': False,\n",
       " 'late-night': False,\n",
       " 'talk': False,\n",
       " 'host': False,\n",
       " 'becomes': False,\n",
       " 'captivated': False,\n",
       " 'harrowing': False,\n",
       " 'young': False,\n",
       " 'listener': False,\n",
       " 'adoptive': False,\n",
       " 'mother': False,\n",
       " 'Toni': False,\n",
       " 'Collette': False,\n",
       " 'troubling': False,\n",
       " 'questions': False,\n",
       " 'arise': False,\n",
       " 'however': False,\n",
       " 'finds': False,\n",
       " 'drawn': False,\n",
       " 'widening': False,\n",
       " 'mystery': False,\n",
       " 'hides': False,\n",
       " 'deadly': False,\n",
       " 'secret': False,\n",
       " 'according': False,\n",
       " 'official': False,\n",
       " 'synopsis.': False,\n",
       " 'You': False,\n",
       " 'really': False,\n",
       " 'STOP': False,\n",
       " 'reading': False,\n",
       " 'comments': False,\n",
       " 'NOW': False,\n",
       " 'How': False,\n",
       " 'lose': False,\n",
       " 'ending': False,\n",
       " 'Ms.': False,\n",
       " 'planning': False,\n",
       " 'chopped': False,\n",
       " 'sent': False,\n",
       " 'deleted': False,\n",
       " 'land': False,\n",
       " 'overkill': False,\n",
       " 'nature': False,\n",
       " 'physical': False,\n",
       " 'mental': False,\n",
       " 'ailments': False,\n",
       " 'obvious': False,\n",
       " 'Mr.': False,\n",
       " 'returns': False,\n",
       " 'New': False,\n",
       " 'York': False,\n",
       " 'Possibly': False,\n",
       " 'blindness': False,\n",
       " 'question': False,\n",
       " '-': False,\n",
       " 'revelation': False,\n",
       " 'certain': False,\n",
       " 'highway': False,\n",
       " 'video': False,\n",
       " 'tape': False,\n",
       " 'would': True,\n",
       " 'benefit': False,\n",
       " 're-editing': False,\n",
       " 'director': False,\n",
       " 'Bobby': False,\n",
       " 'Cannavale': False,\n",
       " 'Jess': False,\n",
       " 'initially': False,\n",
       " 'believable': False,\n",
       " 'couple': False,\n",
       " 'establishing': False,\n",
       " 'relationship': False,\n",
       " 'might': True,\n",
       " 'helped': False,\n",
       " 'set': False,\n",
       " 'stage': False,\n",
       " 'Otherwise': False,\n",
       " 'exemplary': False,\n",
       " 'offers': False,\n",
       " 'exceptionally': False,\n",
       " 'strong': False,\n",
       " 'characterization': False,\n",
       " 'gay': False,\n",
       " 'impersonation': False,\n",
       " 'Anna': False,\n",
       " 'Joe': False,\n",
       " 'Morton': False,\n",
       " 'Ashe': False,\n",
       " 'Pete': False,\n",
       " 'Logand': False,\n",
       " 'perfect.': False,\n",
       " 'Donna': False,\n",
       " 'belongs': False,\n",
       " 'creepy': False,\n",
       " 'hall': False,\n",
       " 'correct': False,\n",
       " 'saying': False,\n",
       " \"'Psycho\": False,\n",
       " 'several': False,\n",
       " 'organizations': False,\n",
       " 'giving': False,\n",
       " 'awards': False,\n",
       " 'seemed': False,\n",
       " 'reach': False,\n",
       " 'women': False,\n",
       " 'due': False,\n",
       " 'slighter': False,\n",
       " 'dispersion': False,\n",
       " 'roles': False,\n",
       " 'certainly': False,\n",
       " 'noticed': False,\n",
       " 'award': False,\n",
       " 'consideration': False,\n",
       " 'She': False,\n",
       " 'And': False,\n",
       " 'Patrick': False,\n",
       " 'Stettner': False,\n",
       " 'evokes': False,\n",
       " 'Hitchcock': False,\n",
       " 'makes': False,\n",
       " 'getting': False,\n",
       " 'sandwich': False,\n",
       " 'vending': False,\n",
       " 'machine': False,\n",
       " 'suspenseful.': False,\n",
       " 'Finally': False,\n",
       " 'writers': False,\n",
       " 'Armistead': False,\n",
       " 'Maupin': False,\n",
       " 'Terry': False,\n",
       " 'Anderson': False,\n",
       " 'deserve': False,\n",
       " 'gratitude': False,\n",
       " 'attendants': False,\n",
       " 'everywhere.': False,\n",
       " '*******': False,\n",
       " 'Night': False,\n",
       " 'Listener': False,\n",
       " '1/21/06': False,\n",
       " '~': False,\n",
       " 'THE': False,\n",
       " 'NIGHT': False,\n",
       " 'LISTENER': False,\n",
       " '2006': False,\n",
       " '**1/2': False,\n",
       " 'John': False,\n",
       " 'Cullum': False,\n",
       " 'Lisa': False,\n",
       " 'Emery': False,\n",
       " 'Becky': False,\n",
       " 'Baker': False,\n",
       " 'Dir': False,\n",
       " 'Hitchcockian': False,\n",
       " 'suspenser': False,\n",
       " 'gives': False,\n",
       " 'stand-out': False,\n",
       " 'low-key': False,\n",
       " 'performance.': False,\n",
       " 'celebrities': False,\n",
       " 'fans': False,\n",
       " 'near': False,\n",
       " 'paranoia': False,\n",
       " 'associates': False,\n",
       " 'almost': False,\n",
       " 'norm': False,\n",
       " 'latest': False,\n",
       " 'derange': False,\n",
       " 'fan': False,\n",
       " 'scenario': False,\n",
       " 'talk-radio': False,\n",
       " 'personality': False,\n",
       " 'named': False,\n",
       " 'No': False,\n",
       " 'reads': False,\n",
       " 'stories': False,\n",
       " 'penned': False,\n",
       " 'airwaves': False,\n",
       " 'accumulated': False,\n",
       " 'interesting': False,\n",
       " 'form': False,\n",
       " 'submitted': False,\n",
       " 'manuscript': False,\n",
       " 'travails': False,\n",
       " 'troubled': False,\n",
       " 'youth': False,\n",
       " 'editor': False,\n",
       " 'read': False,\n",
       " 'naturally': False,\n",
       " 'disturbed': False,\n",
       " 'ultimately': False,\n",
       " 'intrigued': False,\n",
       " 'nightmarish': False,\n",
       " 'existence': False,\n",
       " 'abducted': False,\n",
       " 'sexually': False,\n",
       " 'abused': False,\n",
       " 'finally': False,\n",
       " 'rescued': False,\n",
       " 'nurse': False,\n",
       " 'excellent': False,\n",
       " 'adopted': False,\n",
       " 'correspondence': False,\n",
       " 'reveals': False,\n",
       " 'dying': False,\n",
       " 'AIDS': False,\n",
       " 'Naturally': False,\n",
       " 'meet': False,\n",
       " 'suddenly': False,\n",
       " 'doubt': False,\n",
       " 'possibly': False,\n",
       " 'devious': False,\n",
       " 'ulterior': False,\n",
       " 'motives': False,\n",
       " 'seed': False,\n",
       " 'planted': False,\n",
       " 'estranged': False,\n",
       " 'lover': False,\n",
       " 'whose': False,\n",
       " 'sudden': False,\n",
       " 'departure': False,\n",
       " 'City': False,\n",
       " 'apartment': False,\n",
       " 'emotional': False,\n",
       " 'tailspin': False,\n",
       " 'grown': False,\n",
       " 'tempest': False,\n",
       " 'teacup': False,\n",
       " 'decides': False,\n",
       " 'investigating': False,\n",
       " 'backgrounds': False,\n",
       " 'discovering': False,\n",
       " 'truths': False,\n",
       " 'anticipate.': False,\n",
       " 'Written': False,\n",
       " 'co-wrote': False,\n",
       " 'screenplay': False,\n",
       " 'former': False,\n",
       " 'novice': False,\n",
       " 'hoax': False,\n",
       " 'run': False,\n",
       " 'full': False,\n",
       " 'tilt': False,\n",
       " 'old': False,\n",
       " 'fashioned': False,\n",
       " 'pot-boiler': False,\n",
       " 'helps': False,\n",
       " 'conflicted': False,\n",
       " 'good-hearted': False,\n",
       " 'genuinely': False,\n",
       " 'number': False,\n",
       " 'fact': False,\n",
       " 'thing': False,\n",
       " 'escaped': False,\n",
       " 'unsettling': False,\n",
       " 'dreadful': False,\n",
       " 'trait': False,\n",
       " 'leave': False,\n",
       " 'unmentioned': False,\n",
       " 'underlines': False,\n",
       " 'desperation': False,\n",
       " 'rattle': False,\n",
       " 'core.': False,\n",
       " 'runs': False,\n",
       " 'gas': False,\n",
       " 'eventually': True,\n",
       " 'repetitive': False,\n",
       " 'predictable': False,\n",
       " 'despite': False,\n",
       " 'finely': False,\n",
       " 'directed': False,\n",
       " 'piece': False,\n",
       " 'hoodwink': False,\n",
       " 'pays': False,\n",
       " 'listen': False,\n",
       " 'inner': False,\n",
       " 'voice': False,\n",
       " 'careful': False,\n",
       " 'hope': False,\n",
       " 'God': False,\n",
       " 'bless': False,\n",
       " 'constantly': False,\n",
       " 'shooting': False,\n",
       " 'foot': False,\n",
       " 'lately': False,\n",
       " 'dumb': False,\n",
       " 'done': False,\n",
       " 'decade': False,\n",
       " 'perhaps': False,\n",
       " 'exception': False,\n",
       " 'Death': False,\n",
       " 'To': False,\n",
       " 'Smoochy': False,\n",
       " 'bombed': False,\n",
       " 'came': False,\n",
       " 'cult': False,\n",
       " 'dramas': False,\n",
       " 'especially': False,\n",
       " 'Insomnia': False,\n",
       " 'One': False,\n",
       " 'Hour': False,\n",
       " 'Photo': False,\n",
       " 'mediocre': False,\n",
       " 'reviews': False,\n",
       " 'quick': False,\n",
       " 'DVD': False,\n",
       " 'release': False,\n",
       " 'among': False,\n",
       " 'period.': False,\n",
       " 'chilling': False,\n",
       " 'include': False,\n",
       " 'serial': False,\n",
       " 'killer': False,\n",
       " 'anyone': False,\n",
       " 'physically': False,\n",
       " 'dangerous': False,\n",
       " 'concept': False,\n",
       " 'actual': False,\n",
       " 'case': False,\n",
       " 'fraud': False,\n",
       " 'yet': False,\n",
       " 'officially': False,\n",
       " 'confirmed': False,\n",
       " 'high': False,\n",
       " 'autobiography': False,\n",
       " 'child': False,\n",
       " 'Anthony': False,\n",
       " 'Godby': False,\n",
       " 'Johnson': False,\n",
       " 'suffered': False,\n",
       " 'horrific': False,\n",
       " 'abuse': False,\n",
       " 'contracted': False,\n",
       " 'result': False,\n",
       " 'moved': False,\n",
       " 'reports': False,\n",
       " 'online': False,\n",
       " 'may': False,\n",
       " 'exist': False,\n",
       " 'confused': False,\n",
       " 'feelings': True,\n",
       " 'brilliantly': False,\n",
       " 'portrayed': False,\n",
       " 'resurfaced': False,\n",
       " 'mind.': False,\n",
       " 'sociopathic': False,\n",
       " 'caretaker': False,\n",
       " 'Her': False,\n",
       " 'role': False,\n",
       " 'cry': False,\n",
       " 'Little': False,\n",
       " 'Miss': False,\n",
       " 'Sunshine': False,\n",
       " 'times': False,\n",
       " 'looked': False,\n",
       " 'camera': False,\n",
       " 'thought': False,\n",
       " 'staring': False,\n",
       " 'takes': False,\n",
       " 'play': False,\n",
       " 'sort': False,\n",
       " 'understated': False,\n",
       " 'reviewed': False,\n",
       " 'actresses': False,\n",
       " 'generation': False,\n",
       " 'nominated': False,\n",
       " 'Academy': False,\n",
       " 'Award': False,\n",
       " '2008': False,\n",
       " 'incredible': False,\n",
       " 'least': False,\n",
       " 'scary': False,\n",
       " 'too.': False,\n",
       " 'dark': False,\n",
       " 'recommend': False,\n",
       " 'Be': False,\n",
       " 'prepared': False,\n",
       " 'unsettled': False,\n",
       " 'leaves': False,\n",
       " 'strange': False,\n",
       " 'feeling': False,\n",
       " 'first': False,\n",
       " 'Maupins': False,\n",
       " 'taken': False,\n",
       " 'displayed': False,\n",
       " 'cares': False,\n",
       " 'loves': False,\n",
       " 'said': False,\n",
       " 'version': False,\n",
       " 'expected': False,\n",
       " 'past': False,\n",
       " 'gloss': False,\n",
       " 'Hollywood': False,\n",
       " 'succeeded': False,\n",
       " 'With': False,\n",
       " 'amount': False,\n",
       " 'restraint': False,\n",
       " 'captures': False,\n",
       " 'fragile': False,\n",
       " 'essence': False,\n",
       " 'lets': False,\n",
       " 'us': False,\n",
       " 'struggle': False,\n",
       " 'issues': False,\n",
       " 'trust': False,\n",
       " 'personnel': False,\n",
       " 'around': False,\n",
       " 'As': False,\n",
       " 'introduced': False,\n",
       " 'players': False,\n",
       " 'reminded': False,\n",
       " 'nothing': False,\n",
       " 'seems': False,\n",
       " 'smallest': False,\n",
       " 'event': False,\n",
       " 'change': False,\n",
       " 'lives': False,\n",
       " 'irrevocably': False,\n",
       " 'request': False,\n",
       " 'review': False,\n",
       " 'book': False,\n",
       " 'turns': False,\n",
       " 'changing': False,\n",
       " 'find': False,\n",
       " 'strength': False,\n",
       " 'within': False,\n",
       " 'carry': False,\n",
       " 'forward.': False,\n",
       " 'bad': False,\n",
       " 'avoid': False,\n",
       " 'average': False,\n",
       " 'American': False,\n",
       " 'serious': False,\n",
       " 'PLEASE': False,\n",
       " 'GIVE': False,\n",
       " 'THIS': False,\n",
       " 'MOVIE': False,\n",
       " 'CHANCE': False,\n",
       " 'touches': False,\n",
       " 'darkness': False,\n",
       " 'go': False,\n",
       " 'Like': False,\n",
       " 'stepped': False,\n",
       " 'another': False,\n",
       " 'quality': False,\n",
       " 'art.': False,\n",
       " 'forget': False,\n",
       " 'steals': False,\n",
       " '1940': False,\n",
       " 'leading': False,\n",
       " 'looks': False,\n",
       " 'screen': False,\n",
       " 'presence': False,\n",
       " 'hacks': False,\n",
       " 'opinion': False,\n",
       " 'S~': False,\n",
       " 'liked': False,\n",
       " 'Some': False,\n",
       " 'action': False,\n",
       " 'tense': False,\n",
       " 'opening': True,\n",
       " 'semi': False,\n",
       " 'truck': False,\n",
       " 'done.': False,\n",
       " 'transitional': False,\n",
       " 'filmed': False,\n",
       " 'ways': False,\n",
       " 'lapse': False,\n",
       " 'photography': False,\n",
       " 'unusual': False,\n",
       " 'colors': False,\n",
       " 'evil': False,\n",
       " \"'d\": False,\n",
       " '8': False,\n",
       " '10': False,\n",
       " 'illnesses': False,\n",
       " 'born': False,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 리뷰 데이터의 문장들을 이용해서 feature 생성\n",
    "with open('../../aclImdb/train/neg/{}'.format(files[0]), 'r', encoding='utf-8') as f:\n",
    "    review = nltk.word_tokenize(f.read())\n",
    "find_features(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000개의 긍/부정 리뷰 데이터에 대해 feature set 생성\n",
    "feature_sets = []\n",
    "\n",
    "# train/pos\n",
    "files = os.listdir('../../aclImdb/train/pos')[:1000]\n",
    "for file in files:\n",
    "    with open('../../aclImdb/train/pos/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        review = nltk.word_tokenize(f.read())\n",
    "        feature_sets.append((find_features(review), 'pos'))\n",
    "\n",
    "# train/neg\n",
    "files = os.listdir('../../aclImdb/train/neg')[:1000]\n",
    "for file in files:\n",
    "    with open('../../aclImdb/train/neg/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        review = nltk.word_tokenize(f.read())\n",
    "        feature_sets.append((find_features(review), 'neg'))\n",
    "\n",
    "# test/pos\n",
    "files = os.listdir('../../aclImdb/test/pos')[:1000]\n",
    "for file in files:\n",
    "    with open('../../aclImdb/test/pos/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        review = nltk.word_tokenize(f.read())\n",
    "        feature_sets.append((find_features(review), 'pos'))\n",
    "\n",
    "# test/neg\n",
    "files = os.listdir('../../aclImdb/test/neg')[:1000]\n",
    "for file in files:\n",
    "    with open('../../aclImdb/test/neg/{}'.format(file), 'r', encoding='utf-8') as f:\n",
    "        review = nltk.word_tokenize(f.read())\n",
    "        feature_sets.append((find_features(review), 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = feature_sets[:2000]\n",
    "test_set = feature_sets[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nltk.classify.accuracy(clf, test_set) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Naive Bayes Classification model: 82.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the Naive Bayes Classification model:', round(result, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Doc2Vec\n",
    "- 벡터공간 모형의 일종으로 간단한 신경망 모형을 통해 만들어지는 모델\n",
    "- 문맥을 고려한 분석이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec모델 생성을 위한 gensim 패키지\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing을 위한 nltk 패키지\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "class LabeledLineSentence(object):\n",
    "    \n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield LabeledSentence(words=str(doc).split(), tags=[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "labels_list = []\n",
    "\n",
    "# train dataset 생성\n",
    "files = os.listdir('../../aclImdb/train/pos')[:1000]\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('../../aclImdb/train/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "    review_list.append(review)\n",
    "    labels_list.append('pos_' + file)\n",
    " \n",
    "files = os.listdir('../../aclImdb/train/neg')[:1000]\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('../../aclImdb/train/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "    review_list.append(review)\n",
    "    labels_list.append('neg_' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset 생성\n",
    "files = os.listdir('../../aclImdb/test/pos')[:1000]\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('../../aclImdb/test/pos/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "    review_list.append(review)\n",
    "    labels_list.append('pos_' + file)\n",
    " \n",
    "files = os.listdir('../../aclImdb/test/neg')[:1000]\n",
    "for file in files:\n",
    "    review = ''\n",
    "    with open('../../aclImdb/test/neg/{}'.format(file), 'r', encoding = 'utf-8') as f:\n",
    "        for word in word_tokenize(f.read()):\n",
    "            if lemm.lemmatize(word) not in stop_words:\n",
    "                review += ' ' + word\n",
    "        f.close()\n",
    "    review_list.append(review)\n",
    "    labels_list.append('neg_' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/Users/macbook/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# 리뷰 리스트와 레이블 리스트를 합쳐서 LabelLineSentence 객체를 생성\n",
    "it = LabeledLineSentence(doc_list = review_list, labels_list = labels_list)\n",
    "\n",
    "# Doc2Vec모델을 학습 \n",
    "'''\n",
    "size : feature벡터의 차원\n",
    "window : 예측된 단어와 문맥의 단어들 사이의 최대 거리\n",
    "dm : 트레이닝 알고리즘 (default=distributed memory)\n",
    "alpha : 초기 학습률 (learning rate)\n",
    "min_alpha : alpha값이 학습과정에서 선형으로 줄어들어서 도달하는 최소값\n",
    "min_count 이하의 total frequency를 가진 단어들은 무시\n",
    "workers : cpu의 코어 수에 따라 multi-threads를 지원해서 병렬처리하는 옵션\n",
    "그 밖의 옵션은  https://radimrehurek.com/gensim/models/doc2vec.html 참고\n",
    "'''\n",
    "\n",
    "model = Doc2Vec(size=3000, window=10, dm=0, alpha=0.025, \\\n",
    "                min_alpha=0.025, min_count=5, workers=multiprocessing.cpu_count())\n",
    "model.build_vocab(it)\n",
    "model.train(it, total_examples=4000, epochs=20)\n",
    "model.save('partial_Doc2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('partial_Doc2Vec.model')\n",
    "\n",
    "x_train = np.zeros((2000, 3000))\n",
    "y_train = np.zeros(2000)\n",
    "x_test = np.zeros((2000, 3000))\n",
    "y_test = np.zeros(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../../aclImdb/train/pos')[:1000]\n",
    "for i in range(1000):\n",
    "    x_train[i] = model.docvecs['pos_' + files[i]]\n",
    "    y_train[i] = 1\n",
    "files = os.listdir('../../aclImdb/train/neg')[:1000]\n",
    "for i in range(1000):\n",
    "    x_train[i+1000] = model.docvecs['neg_' + files[i]]\n",
    "    y_train[i+1000] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../../aclImdb/test/pos')[:1000]\n",
    "for i in range(1000):\n",
    "    x_test[i] = model.docvecs['pos_' + files[i]]\n",
    "    y_test[i] = 1\n",
    "files = os.listdir('../../aclImdb/test/neg')[:1000]\n",
    "for i in range(1000):\n",
    "    x_test[i+1000] = model.docvecs['neg_' + files[i]]\n",
    "    y_test[i+1000] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커널이 자꾸 죽음.... 약 91.65%의 정확도를 보인다고 함\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras 의 LSTM (Long Short-Term Memory)을 이용한 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# keras에 imdb 데이터가 이미 전처리되어 저장되어 있음\n",
    "# 그대로 불러올 때, 상위 5000개의 단어만 활용\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 형태를 바꿔줌\n",
    "max_len = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)[:2000, :]\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)[:2000, :]\n",
    "y_train = y_train[:2000]\n",
    "y_test = y_test[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length = max_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2000/2000 [==============================] - 25s 13ms/step - loss: 0.6918 - acc: 0.5350\n",
      "Epoch 2/3\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.6541 - acc: 0.7560\n",
      "Epoch 3/3\n",
      "2000/2000 [==============================] - 24s 12ms/step - loss: 0.5539 - acc: 0.7935\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.6 %\n"
     ]
    }
   ],
   "source": [
    "print(scores[1] * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
